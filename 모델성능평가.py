# -*- coding: utf-8 -*-
"""모델 성능 평가.ipynb의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F6HnF0RUUZiEQWiHG-W4JCXG2gpkCEwi
"""

!pip install numpy scipy

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/facebookresearch/demucs.git
# %cd demucs
!pip install -e .

!pip install --upgrade pip
!pip uninstall -y tensorflow
!pip install tensorflow==2.9.1
!pip install spleeter==2.4.0
!pip install librosa
!pip install mir_eval
!pip install soundfile
pip install demucs

import os
import numpy as np
import librosa
import torch
import soundfile as sf
from mir_eval.separation import bss_eval_sources
from demucs.pretrained import get_model
from demucs.apply import apply_model

# CUDA 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Demucs 모델 로드
def load_demucs_model():
    print("Loading Demucs model...")
    model = get_model("htdemucs")
    model.to(device)
    model.eval()
    print("Demucs model loaded successfully.")
    return model

demucs_model = load_demucs_model()

# Demucs 평가 함수
def evaluate_demucs(mix_path, vocal_path, background_path, model):
    try:
        print(f"Evaluating: {mix_path}")

        # 믹스 오디오 로드
        mix_audio, sr = librosa.load(mix_path, sr=44100, mono=False)
        mix_audio = torch.tensor(mix_audio, dtype=torch.float32).unsqueeze(0).to(device)
        print(f"Loaded mix audio: {mix_path}, Shape: {mix_audio.shape}, Sample rate: {sr}")

        # Demucs로 분리
        with torch.no_grad():
            sources = apply_model(model, mix_audio, device=device)[0].cpu().numpy()

        # 소스 매핑
        predicted_vocals = sources[3]  # Vocals
        predicted_accompaniment = sources[:3].sum(axis=0)  # 나머지 소스 합산

        print(f"Demucs output shape: {sources.shape}")

        # 분리된 오디오 저장 (디버깅용)
        sf.write("vocals_debug.wav", predicted_vocals.T, sr)
        sf.write("accompaniment_debug.wav", predicted_accompaniment.T, sr)
        print("Separated audio files saved for debugging.")

        # 원본 오디오 로드
        original_vocals, _ = librosa.load(vocal_path, sr=sr, mono=False)
        original_background, _ = librosa.load(background_path, sr=sr, mono=False)

        # 최소 길이 맞추기
        min_length = min(
            predicted_vocals.shape[-1],
            predicted_accompaniment.shape[-1],
            original_vocals.shape[-1],
            original_background.shape[-1],
        )
        predicted_vocals = predicted_vocals[..., :min_length]
        predicted_accompaniment = predicted_accompaniment[..., :min_length]
        original_vocals = original_vocals[..., :min_length]
        original_background = original_background[..., :min_length]

        # 차원 변환: (소스 수, 샘플 수)
        estimated_sources = np.stack([predicted_vocals.sum(axis=0), predicted_accompaniment.sum(axis=0)])
        reference_sources = np.stack([original_vocals.sum(axis=0), original_background.sum(axis=0)])
        print(f"Estimated sources shape: {estimated_sources.shape}")
        print(f"Reference sources shape: {reference_sources.shape}")

        # 평가 수행
        sdr, sir, sar, _ = bss_eval_sources(reference_sources, estimated_sources)

        return np.mean(sdr), np.mean(sir), np.mean(sar)
    except Exception as e:
        print(f"Demucs evaluation failed: {e}")
        return None, None, None

# 평가 실행 함수
def batch_evaluate(mix_files, vocal_files, background_files):
    results = {"sdr": [], "sir": [], "sar": []}

    for mix_path, vocal_path, background_path in zip(mix_files, vocal_files, background_files):
        sdr, sir, sar = evaluate_demucs(mix_path, vocal_path, background_path, demucs_model)
        if sdr is not None:
            results["sdr"].append(sdr)
            results["sir"].append(sir)
            results["sar"].append(sar)

    # 최종 결과 계산
    final_results = {
        "SDR": np.nanmean(results["sdr"]),
        "SIR": np.nanmean(results["sir"]),
        "SAR": np.nanmean(results["sar"]),
    }
    print("--- Demucs 최종 결과 ---")
    print(final_results)
    return final_results

# 메인 실행 부분
if __name__ == "__main__":
    mix_files = ["/content/mix_James May - If You Say.wav"]
    vocal_files = ["/content/vocals_James May - If You Say.wav"]
    background_files = ["/content/backgrounds_James May - If You Say.wav"]

    batch_evaluate(mix_files, vocal_files, background_files)

import os
import librosa
import numpy as np
from spleeter.separator import Separator
from mir_eval.separation import bss_eval_sources

# Spleeter 모델 로드
separator = Separator("spleeter:2stems", MWF=False)

def evaluate_spleeter(mix_path, vocal_path, background_path, output_dir="/content/separated"):
    try:
        # 결과 디렉토리 생성
        os.makedirs(output_dir, exist_ok=True)

        # Spleeter로 오디오 분리
        print(f"Separating audio using Spleeter for: {mix_path}")
        separator.separate_to_file(mix_path, output_dir)

        # 결과 파일 경로
        base_name = os.path.splitext(os.path.basename(mix_path))[0]
        predicted_vocal_path = os.path.join(output_dir, base_name, "vocals.wav")
        predicted_background_path = os.path.join(output_dir, base_name, "accompaniment.wav")

        # 분리된 오디오 파일 확인
        if not os.path.exists(predicted_vocal_path) or not os.path.exists(predicted_background_path):
            print(f"Predicted files not found: {predicted_vocal_path}, {predicted_background_path}")
            return None, None, None

        # 분리된 오디오 로드
        predicted_vocals, sr_vocals = librosa.load(predicted_vocal_path, sr=None)
        predicted_background, sr_background = librosa.load(predicted_background_path, sr=None)

        # 원본 오디오 로드
        original_vocals, sr_original_vocals = librosa.load(vocal_path, sr=None)
        original_background, sr_original_background = librosa.load(background_path, sr=None)

        # 샘플레이트 확인 및 정렬
        if sr_vocals != sr_original_vocals or sr_background != sr_original_background:
            print(f"Sample rates do not match: {sr_vocals}, {sr_original_vocals}")
            return None, None, None

        # 길이 동기화
        min_length = min(
            len(predicted_vocals),
            len(predicted_background),
            len(original_vocals),
            len(original_background),
        )
        predicted_vocals = predicted_vocals[:min_length]
        predicted_background = predicted_background[:min_length]
        original_vocals = original_vocals[:min_length]
        original_background = original_background[:min_length]

        print(f"Aligned lengths: {min_length}")

        # 평가를 위한 소스 스택
        estimated_sources = np.stack([predicted_vocals, predicted_background])
        reference_sources = np.stack([original_vocals, original_background])

        print(f"Estimated sources shape: {estimated_sources.shape}")
        print(f"Reference sources shape: {reference_sources.shape}")

        # SDR, SIR, SAR 계산
        sdr, sir, sar, _ = bss_eval_sources(reference_sources, estimated_sources)
        print(f"Spleeter Evaluation Completed - SDR: {sdr.mean()}, SIR: {sir.mean()}, SAR: {sar.mean()}")

        return sdr.mean(), sir.mean(), sar.mean()

    except Exception as e:
        print(f"Spleeter evaluation failed: {e}")
        return None, None, None

# 실행
if __name__ == "__main__":
    mix_path = "/content/mix_James May - If You Say.wav"
    vocal_path = "/content/vocals_James May - If You Say.wav"
    background_path = "/content/backgrounds_James May - If You Say.wav"

    sdr, sir, sar = evaluate_spleeter(mix_path, vocal_path, background_path)
    print(f"Spleeter Results - SDR: {sdr}, SIR: {sir}, SAR: {sar}")

